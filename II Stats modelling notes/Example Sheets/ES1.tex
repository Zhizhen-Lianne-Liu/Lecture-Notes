\documentclass{article}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{amsthm}
\usepackage{enumerate}



\DeclareMathOperator{\bX}{\boldsymbol{X}}
\DeclareMathOperator{\iid}{\stackrel{iid}{\sim}}
\DeclareMathOperator{\hbb}{\hat{\boldsymbol{\beta}}}
\DeclareMathOperator{\bY}{\boldsymbol{Y}}
\DeclareMathOperator{\bH}{\boldsymbol{H}}
\DeclareMathOperator{\hbmu}{\hat{\boldsymbol{\mu}}}
\DeclareMathOperator{\bmu}{\boldsymbol{\mu}}
\DeclareMathOperator{\real}{\mathbb{R}}
\DeclareMathOperator{\bP}{\boldsymbol{P}}
\DeclareMathOperator{\bZ}{\boldsymbol{Z}}
\DeclareMathOperator{\bSigma}{\boldsymbol{\Sigma}}
\DeclareMathOperator{\bbeta}{\boldsymbol{\beta}}
\newcommand{\indep}{\perp \!\!\! \perp}

\begin{document}

\begin{center}\LARGE\bf
	Stats Modelling Example Sheet 1
\end{center}

In all the questions that follow, $ X $ is an n by p design matrix with full column rank and $ H $ is the orthogonal projection onto the column space of $ X $. Also, let $ X_0 $ be the matrix formed from the first $ p_0 < p $ columns of $ X $ and let $ H_0 $ be the orthogonal projection on to the column space of $ X_0 $. The vector $ Y \in \real^n $ will be a vector of responses and we will define $ \hbb := (X^TX)^{-1}X^TY $, $ \hbb_0 :=(X_0^T X_0)^{-1}X_0^T Y $ and $ \tilde{\sigma}^2 := ||(I-H)Y||^2/(n-p)$. By normal linear model, we mean the model $ Y = X\beta + \epsilon $, $ \epsilon | X \sim N_n(0, \sigma^2 I) $.

\section{Question 1}
\fbox{\begin{minipage}{42em}
	Show that
	\[||(H-H_0)Y||^2 = ||(I-H_0)Y||^2 - ||(I-H)Y||^2 = ||HY||^2 - ||H_oY||^2\]
	
\end{minipage}}
\bigskip \\
We know that since $ H $ and $ H_0 $ are projection matrices, they have the properties: $ H_0 H = HH_0 = H_0 $ and $ H^T = H $, $ H_0^T = H_0 $

\begin{align}
	||(H-H_0)Y||^2 & = Y^T(H-H_0)^T(H-H_0)Y \\
	& = Y^T[H^TH - H_0^TH - H^TH_0 + H_0^TH_0]Y \\
	& = Y^T[H - H_0 - H_0 + H_0]Y \\
	& = Y^T[H-H_0]Y
\end{align}

\begin{align}
	||(I-H_0)Y||^2 - ||(I-H)Y||^2  & = ((I-H_0)Y - (I-H)Y)^T((I-H_0)Y + (I-H)Y) \\
	& = Y^T(H-H_0)^T(2I-H-H_0)Y \\
	& = Y^T(2H^T - 2H_0^T - H^TH + H_0^TH - H^TH_0 + H_0^TH_0)Y \\
	& = Y^T[H-H_0]Y
\end{align}

\begin{align}
	||HY||^2 - ||H_0Y||^2 & = ((H-H_0)Y)^T((H+H_0)Y) \\
	& = Y^T(H^TH - H_0^TH + H^TH_0 - H_0^TH_0)Y \\
	& = Y^T(H-H_0)Y
\end{align}

\section{Question 2}
\fbox{\begin{minipage}{42em}
		Let $ \sigma $ be a known $ n \times n $ positive definite matrix. Consider the following linear model
		\[Y = X \beta + \epsilon, \qquad \epsilon|X \sim N(0, \sigma^2 \Sigma)\]
		where $ \beta $ and $ \sigma^2 $ are unknown. Let $ \hbb_{\Sigma} $ denote the maximum likelihood estimator of $ \beta $. What optimisation problem does $ \hbb_{\Sigma} $ solve? Show that if $ (X^T \Sigma^{-1}X) $ is invertible, $ \hbb_{\Sigma} $ is given by 
		\[\hbb_{\Sigma} = (X^T \Sigma^{-1}X)^{-1} X^T \Sigma^{-1}Y\]
		This is called the generalised least squares estimator. Show that $ \hbb_{\Sigma} $ is the best linear unbiased estimator (BLUE) in this model. (3)
\end{minipage}}
\bigskip \\
%need to try again with hint in notes I didn't notice before


\section{Question 3}

\fbox{\begin{minipage}{42em}
		Consider the model $ Y = \mu + \epsilon $ where $ \mathbb{E}(\epsilon) = 0 $ and $ Var(\epsilon) = \sigma^2 I $ and $ \mu \in \real^n $ is a non-random vector. Suppose we have performed least squares of $ Y $ on a foxed design matrix X, so the fitted valies are HY. Show that if $ Y^* = \mu + \epsilon^* $ where $ \mathbb{E}(\epsilon^*) = 0 $ and $ Var(\epsilon^*) = \sigma^2 I $ and $ \epsilon^* \indep \epsilon $, then
		\[\frac{1}{n} \mathbb{E}(||HY - Y^*||^2) = \sigma^2 + \frac{1}{n}||(I-H)\mu||^2 + \frac{\sigma^2 p}{n}\]
		This is known as the bias-variance tradeoff, as a larger model reduces the second term ($ "bias^2" $) but increases the third term ("variance").
\end{minipage}}
\bigskip \\
\begin{align}
	\frac{1}{n}\mathbb{E}(||HY-Y^*||^2) = 
\end{align}
\section{Question 4}



\end{document}